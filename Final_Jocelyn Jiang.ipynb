{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6b5254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jocel\\spark\\spark-4.0.1-bin-hadoop3\\\n",
      "4.0.1\n"
     ]
    }
   ],
   "source": [
    "# Setup from Lab 4\n",
    "import os\n",
    "import json\n",
    "import numpy\n",
    "import datetime\n",
    "import certifi\n",
    "import pandas as pd\n",
    "\n",
    "import pymongo\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Setup from Lab 6\n",
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "import pymysql\n",
    "from delta import *\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fedc746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "# mysql_args = {\n",
    "#     \"host_name\" : \"localhost\",\n",
    "#     \"port\" : \"3306\",\n",
    "#     \"db_name\" : \"northwind_dw2\",\n",
    "#     \"conn_props\" : {\n",
    "#         \"user\" : \"root\",\n",
    "#         \"password\" : \"Zhu0z!m!m@37*\",\n",
    "#         \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "#     }\n",
    "# }\n",
    "mysql_args = {\n",
    "    \"uid\" : \"root\",\n",
    "    \"pwd\" : \"Zhu0z!m!m%4037*\",\n",
    "    \"hostname\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"dbname\" : \"adventureworks\",\n",
    "    \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"atlas\",\n",
    "    \"user_name\" : \"jocelynjiang1\",\n",
    "    \"password\" : \"Zhu0z!m!m%4037*\",\n",
    "    \"cluster_name\" : \"cluster0\",\n",
    "    \"cluster_subnet\" : \"cqza4jd\",\n",
    "    \"db_name\" : \"adventureworks\",\n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c2a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    '''Drop Columns having a percentage of NULL values that exceeds the given 'threshold' parameter value.'''\n",
    "    columns_with_nulls = [col for col in df.columns if df.filter(df[col].isNull()).count() / df.count() > threshold] \n",
    "    df_dropped = df.drop(*columns_with_nulls) \n",
    "    \n",
    "    return df_dropped\n",
    "    \n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['hostname']}:{args['port']}/{args['dbname']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['driver']) \\\n",
    "    .option(\"user\", args['uid']) \\\n",
    "    .option(\"password\", args['pwd']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Northwind Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.13:10.4.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "    \n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    # dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    mongo_uri = get_mongo_uri(**mongodb_args)\n",
    "    dframe = spark_session.read.format(\"mongodb\") \\\n",
    "        .option(\"connection.uri\", mongo_uri) \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']) \\\n",
    "            .load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe\n",
    "\n",
    "\n",
    "def get_sql_dataframe(sql_query, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the pd.read_sql() function to query the database, and fill a Pandas DataFrame.'''\n",
    "    dframe = pd.read_sql(text(sql_query), connection);\n",
    "    connection.close()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def set_dataframe(df, table_name, pk_column, db_operation, **args):\n",
    "    '''Create a connection to the MySQL database'''\n",
    "    conn_str = f\"mysql+pymysql://{args['uid']}:{args['pwd']}@{args['hostname']}/{args['dbname']}\"\n",
    "    sqlEngine = create_engine(conn_str, pool_recycle=3600)\n",
    "    connection = sqlEngine.connect()\n",
    "    \n",
    "    '''Invoke the Pandas DataFrame .to_sql( ) function to either create, or append to, a table'''\n",
    "    if db_operation == \"insert\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='replace')\n",
    "        connection.execute(text(f\"ALTER TABLE {table_name} ADD PRIMARY KEY ({pk_column});\"))\n",
    "            \n",
    "    elif db_operation == \"update\":\n",
    "        df.to_sql(table_name, con=connection, index=False, if_exists='append')\n",
    "    \n",
    "    connection.close()\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the cluster_location parameter.\")\n",
    "    \n",
    "    else:\n",
    "        if args[\"cluster_location\"] == \"atlas\":\n",
    "            connect_str = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "            connect_str += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net\"\n",
    "            client = pymongo.MongoClient(connect_str, tlsCAFile=certifi.where())\n",
    "            \n",
    "        elif args[\"cluster_location\"] == \"local\":\n",
    "            client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "        \n",
    "    return client\n",
    "\n",
    "\n",
    "def get_mongo_dataframe(mongo_client, db_name, collection, query):\n",
    "    '''Query MongoDB, and fill a python list with documents to create a DataFrame'''\n",
    "    db = mongo_client[db_name]\n",
    "    dframe = pd.DataFrame(list(db[collection].find(query)))\n",
    "    dframe.drop(['_id'], axis=1, inplace=True)\n",
    "    mongo_client.close()\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b12126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DateKey</th>\n",
       "      <th>Date</th>\n",
       "      <th>Day</th>\n",
       "      <th>DaySuffix</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>WeekDayName</th>\n",
       "      <th>WeekDayName_Short</th>\n",
       "      <th>WeekDayName_FirstLetter</th>\n",
       "      <th>DOWInMonth</th>\n",
       "      <th>DayOfYear</th>\n",
       "      <th>...</th>\n",
       "      <th>LastDateofQuater</th>\n",
       "      <th>FirstDateofMonth</th>\n",
       "      <th>LastDateofMonth</th>\n",
       "      <th>FirstDateofWeek</th>\n",
       "      <th>LastDateofWeek</th>\n",
       "      <th>CurrentYear</th>\n",
       "      <th>CurrentQuater</th>\n",
       "      <th>CurrentMonth</th>\n",
       "      <th>CurrentWeek</th>\n",
       "      <th>CurrentDay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000101</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>st</td>\n",
       "      <td>7</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>SAT</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>1999-12-26</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>-25</td>\n",
       "      <td>-103</td>\n",
       "      <td>-309</td>\n",
       "      <td>-1347</td>\n",
       "      <td>-9427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000102</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2</td>\n",
       "      <td>nd</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>SUN</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2025-12-31</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>2000-01-08</td>\n",
       "      <td>-25</td>\n",
       "      <td>-103</td>\n",
       "      <td>-309</td>\n",
       "      <td>-1346</td>\n",
       "      <td>-9426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    DateKey        Date  Day DaySuffix  Weekday WeekDayName WeekDayName_Short  \\\n",
       "0  20000101  2000-01-01    1        st        7    Saturday               SAT   \n",
       "1  20000102  2000-01-02    2        nd        1      Sunday               SUN   \n",
       "\n",
       "  WeekDayName_FirstLetter  DOWInMonth  DayOfYear  ...  LastDateofQuater  \\\n",
       "0                       S           1          1  ...        2025-12-31   \n",
       "1                       S           2          2  ...        2025-12-31   \n",
       "\n",
       "   FirstDateofMonth  LastDateofMonth FirstDateofWeek LastDateofWeek  \\\n",
       "0        2000-01-01       2000-01-31      1999-12-26     2000-01-01   \n",
       "1        2000-01-01       2000-01-31      2000-01-02     2000-01-08   \n",
       "\n",
       "  CurrentYear  CurrentQuater CurrentMonth  CurrentWeek  CurrentDay  \n",
       "0         -25           -103         -309        -1347       -9427  \n",
       "1         -25           -103         -309        -1346       -9426  \n",
       "\n",
       "[2 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Date dimension: ran provided script in SSMS, exported result as csv, going to read into pd df\n",
    "\n",
    "# SOURCE 1 -- local file system\n",
    "df_dim_date = pd.read_csv('./data/dim_date.csv')\n",
    "dataframe = df_dim_date\n",
    "table_name = 'dim_date'\n",
    "primary_key = 'DateKey'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)\n",
    "\n",
    "df_dim_date.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9c0acad",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_mongo_client(**mongodb_args)\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "# results of dim_employee_vw from AdventureWorks_Queries saved to json; save json to mongo\n",
    "# can query other data on salesperson and salesorderdetails with mysql\n",
    "json_files = {\"employee_details\" : 'employee_details_view.json',\n",
    "             }\n",
    "\n",
    "# SOURCE 2 -- mongoDB (this is the setting step; usage step is later)\n",
    "set_mongo_collections(client, mongodb_args[\"db_name\"], data_dir, json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99fcb4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact_salesorder_key</th>\n",
       "      <th>SalesOrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>ContactID</th>\n",
       "      <th>SalesPersonID</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>BillToAddressID</th>\n",
       "      <th>ShipToAddressID</th>\n",
       "      <th>ShipMethodID</th>\n",
       "      <th>CreditCardID</th>\n",
       "      <th>...</th>\n",
       "      <th>Freight</th>\n",
       "      <th>TotalDue</th>\n",
       "      <th>Comment</th>\n",
       "      <th>ModifiedDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>SalesPersonAge</th>\n",
       "      <th>SalesLastYear</th>\n",
       "      <th>OrderDateKey</th>\n",
       "      <th>DueDateKey</th>\n",
       "      <th>ShipDateKey</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>43659</td>\n",
       "      <td>676</td>\n",
       "      <td>378</td>\n",
       "      <td>279.0</td>\n",
       "      <td>5</td>\n",
       "      <td>985</td>\n",
       "      <td>985</td>\n",
       "      <td>5</td>\n",
       "      <td>16281.0</td>\n",
       "      <td>...</td>\n",
       "      <td>616.0984</td>\n",
       "      <td>27231.5495</td>\n",
       "      <td>None</td>\n",
       "      <td>2001-07-08</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.849641e+06</td>\n",
       "      <td>20010701</td>\n",
       "      <td>20010713</td>\n",
       "      <td>20010708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>43660</td>\n",
       "      <td>117</td>\n",
       "      <td>216</td>\n",
       "      <td>279.0</td>\n",
       "      <td>5</td>\n",
       "      <td>921</td>\n",
       "      <td>921</td>\n",
       "      <td>5</td>\n",
       "      <td>5618.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.8276</td>\n",
       "      <td>1716.1794</td>\n",
       "      <td>None</td>\n",
       "      <td>2001-07-08</td>\n",
       "      <td>Sales Representative</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.849641e+06</td>\n",
       "      <td>20010701</td>\n",
       "      <td>20010713</td>\n",
       "      <td>20010708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fact_salesorder_key  SalesOrderID  CustomerID  ContactID  SalesPersonID  \\\n",
       "0                    1         43659         676        378          279.0   \n",
       "1                    2         43660         117        216          279.0   \n",
       "\n",
       "   TerritoryID  BillToAddressID  ShipToAddressID  ShipMethodID  CreditCardID  \\\n",
       "0            5              985              985             5       16281.0   \n",
       "1            5              921              921             5        5618.0   \n",
       "\n",
       "   ...   Freight    TotalDue  Comment  ModifiedDate                 Title  \\\n",
       "0  ...  616.0984  27231.5495     None    2001-07-08  Sales Representative   \n",
       "1  ...   38.8276   1716.1794     None    2001-07-08  Sales Representative   \n",
       "\n",
       "   SalesPersonAge SalesLastYear OrderDateKey DueDateKey  ShipDateKey  \n",
       "0            37.0  1.849641e+06     20010701   20010713     20010708  \n",
       "1            37.0  1.849641e+06     20010701   20010713     20010708  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insight 1 that I want to observe: average sales last year for a sales representative given their age\n",
    "\n",
    "# ***Assumption for this project: SalespersonID corresponds with EmployeeID. \n",
    "#   I assume this because all SalespersonIDs, when treated as EmployeeIDs, correspond with \n",
    "#   an Employee Title that is a sales-related role\n",
    "\n",
    "# Step 1, going to insert age of salesperson (at time of sale) column into salesorderheader (fact table)\n",
    "# by subtracting employee BirthDate and sales OrderDate\n",
    "\n",
    "# SOURCE 3 -- RDBMS (MySQL)\n",
    "sql_dim_employee = \"SELECT EmployeeID, Title, BirthDate FROM employee;\"\n",
    "df_dim_employee = get_sql_dataframe(sql_dim_employee, **mysql_args)\n",
    "\n",
    "sql_fact_salesorder = \"SELECT * FROM salesorderheader;\"\n",
    "df_fact_salesorder = get_sql_dataframe(sql_fact_salesorder, **mysql_args)\n",
    "df_fact_salesorder.insert(0, \"fact_salesorder_key\", range(1, df_fact_salesorder.shape[0]+1))\n",
    "df_fact_salesorder.drop(columns=['RevisionNumber','Status','OnlineOrderFlag','SalesOrderNumber','PurchaseOrderNumber','AccountNumber','rowguid'], inplace=True)\n",
    "\n",
    "df_fact_salesorder = pd.merge(df_fact_salesorder,df_dim_employee, left_on=\"SalesPersonID\", right_on=\"EmployeeID\",how=\"inner\")\n",
    "df_fact_salesorder.insert(df_fact_salesorder.shape[1], \"SalesPersonAge\", \n",
    "                          ((df_fact_salesorder[\"OrderDate\"]-df_fact_salesorder[\"BirthDate\"]).dt.days/365).round()) # looked up how to treat timestamp difference as days, round age to whole num\n",
    "\n",
    "df_fact_salesorder.drop(columns=['EmployeeID','BirthDate'],inplace=True)\n",
    "\n",
    "\n",
    "# Step 2, add SalesLastYear data for each order's salesperson (includes repeats if multiple sales by one person)\n",
    "sql_dim_salesperson = \"SELECT SalesPersonID, SalesLastYear FROM salesperson;\"\n",
    "df_dim_salesperson = get_sql_dataframe(sql_dim_salesperson, **mysql_args)\n",
    "\n",
    "df_fact_salesorder = pd.merge(df_fact_salesorder, df_dim_salesperson, on=\"SalesPersonID\", how=\"left\")\n",
    "\n",
    "# finally, modify dates to use datekey from date dim\n",
    "# first, update dim_date to keep only date key and date cols:\n",
    "sql_dim_date = \"SELECT DateKey, Date FROM dim_date;\"\n",
    "df_dim_date = get_sql_dataframe(sql_dim_date, **mysql_args)\n",
    "\n",
    "df_fact_salesorder.OrderDate = df_fact_salesorder.OrderDate.astype('datetime64[ns]').dt.date\n",
    "df_fact_salesorder.DueDate = df_fact_salesorder.DueDate.astype('datetime64[ns]').dt.date\n",
    "df_fact_salesorder.ShipDate = df_fact_salesorder.ShipDate.astype('datetime64[ns]').dt.date\n",
    "df_dim_date.Date = df_dim_date.Date.astype('datetime64[ns]').dt.date\n",
    "\n",
    "#\n",
    "df_fact_salesorder = pd.merge(df_fact_salesorder, df_dim_date, left_on=\"OrderDate\",right_on=\"Date\",how=\"inner\")\n",
    "df_fact_salesorder = df_fact_salesorder.rename(columns={'DateKey':'OrderDateKey'})\n",
    "df_fact_salesorder.drop(columns=['OrderDate','Date'],inplace=True)\n",
    "\n",
    "df_fact_salesorder = pd.merge(df_fact_salesorder, df_dim_date, left_on=\"DueDate\",right_on=\"Date\",how=\"inner\")\n",
    "df_fact_salesorder = df_fact_salesorder.rename(columns={'DateKey':'DueDateKey'})\n",
    "df_fact_salesorder.drop(columns=['DueDate','Date'],inplace=True)\n",
    "\n",
    "df_fact_salesorder = pd.merge(df_fact_salesorder, df_dim_date, left_on=\"ShipDate\",right_on=\"Date\",how=\"inner\")\n",
    "df_fact_salesorder = df_fact_salesorder.rename(columns={'DateKey':'ShipDateKey'})\n",
    "df_fact_salesorder.drop(columns=['ShipDate','Date'],inplace=True)\n",
    "\n",
    "# put the fact table into MySQL\n",
    "dataframe = df_fact_salesorder\n",
    "table_name = 'fact_salesorder'\n",
    "primary_key = 'fact_salesorder_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)\n",
    "\n",
    "df_fact_salesorder.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35578fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    SalesPersonAge  avg_sales_last_year\n",
      "0             31.0         1.439156e+06\n",
      "1             32.0         1.439156e+06\n",
      "2             33.0         1.439156e+06\n",
      "3             34.0         1.439156e+06\n",
      "4             35.0         0.000000e+00\n",
      "5             36.0         1.472979e+05\n",
      "6             37.0         1.919542e+06\n",
      "7             38.0         1.785199e+06\n",
      "8             39.0         1.630044e+06\n",
      "9             40.0         1.699203e+06\n",
      "10            42.0         2.177950e+06\n",
      "11            43.0         1.967816e+06\n",
      "12            44.0         1.806209e+06\n",
      "13            45.0         1.769684e+06\n",
      "14            46.0         1.800747e+06\n",
      "15            48.0         2.038235e+06\n",
      "16            49.0         2.014531e+06\n",
      "17            50.0         2.014251e+06\n",
      "18            51.0         1.997186e+06\n",
      "19            52.0         1.997186e+06\n",
      "20            55.0         0.000000e+00\n",
      "21            56.0         0.000000e+00\n",
      "22            57.0         0.000000e+00\n",
      "23            60.0         0.000000e+00\n",
      "24            61.0         0.000000e+00\n",
      "25            62.0         0.000000e+00\n",
      "26            63.0         0.000000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Results analysis:\\n    Values of zero in avg_sales_last_year column likely mean there were salespeople with that age,\\n    but they did not create any sales in the date range of this database.\\n\\n    Generally, the average sales last year increased as age of the salesperson increased.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insight #1, continued:\n",
    "\n",
    "# Now query to analyze sales last year given sales person age\n",
    "# ***(Possible that same salesperson could be represented in two diff age groups\n",
    "#   if salesperson had a birthday, but keeping salesperson repeats \n",
    "#   still lets us see how sales success correlated with age)\n",
    "\n",
    "sql_age_sales_results = \"\"\"\n",
    "    SELECT \n",
    "        SalesPersonAge, \n",
    "        AVG(SalesLastYear) AS avg_sales_last_year\n",
    "    FROM fact_salesorder\n",
    "    GROUP BY SalesPersonAge\n",
    "    ORDER BY SalesPersonAge\n",
    "\"\"\"\n",
    "\n",
    "df_age_sales = get_sql_dataframe(sql_age_sales_results, **mysql_args)\n",
    "print(df_age_sales)\n",
    "\n",
    "\"\"\" Results analysis:\n",
    "    Values of zero in avg_sales_last_year column likely mean there were salespeople with that age,\n",
    "    but they did not create any sales in the date range of this database.\n",
    "\n",
    "    Generally, the average sales last year increased as age of the salesperson increased.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d60c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insight 2: How popularity of products changes with their price\n",
    "sql_fact_productcosthist = \"SELECT * FROM productcosthistory;\"\n",
    "df_fact_productcosthist = get_sql_dataframe(sql_fact_productcosthist, **mysql_args)\n",
    "df_fact_productcosthist.insert(0, \"fact_productcosthist_key\", range(1, df_fact_productcosthist.shape[0]+1))\n",
    "df_fact_productcosthist.drop(columns=['ModifiedDate'],inplace=True)\n",
    "\n",
    "# convert dates to universal keys\n",
    "df_fact_productcosthist.StartDate = df_fact_productcosthist.StartDate.astype('datetime64[ns]').dt.date\n",
    "df_fact_productcosthist.EndDate = df_fact_productcosthist.EndDate.astype('datetime64[ns]').dt.date\n",
    "\n",
    "df_fact_productcosthist = pd.merge(df_fact_productcosthist, df_dim_date, left_on=\"StartDate\", right_on=\"Date\", how=\"inner\")\n",
    "df_fact_productcosthist = df_fact_productcosthist.rename(columns={\"DateKey\":\"StartDateKey\"})\n",
    "df_fact_productcosthist.drop(columns=['StartDate','Date'],inplace=True)\n",
    "\n",
    "df_fact_productcosthist = pd.merge(df_fact_productcosthist, df_dim_date, left_on=\"EndDate\", right_on=\"Date\", how=\"inner\")\n",
    "df_fact_productcosthist = df_fact_productcosthist.rename(columns={\"DateKey\":\"EndDateKey\"})\n",
    "df_fact_productcosthist.drop(columns=['EndDate','Date'],inplace=True)\n",
    "\n",
    "\n",
    "# add product name to fact table for reference\n",
    "sql_dim_product = \"SELECT ProductID, Name FROM product;\"\n",
    "df_dim_product = get_sql_dataframe(sql_dim_product, **mysql_args)\n",
    "df_fact_productcosthist = pd.merge(df_fact_productcosthist, df_dim_product, on='ProductID', how='inner')\n",
    "df_fact_productcosthist.head(2)\n",
    "\n",
    "# put costhist fact table in MySQL for analytics\n",
    "dataframe = df_fact_productcosthist\n",
    "table_name = 'fact_productcosthist'\n",
    "primary_key = 'fact_productcosthist_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)\n",
    "\n",
    "# add in transaction date info from transaction history\n",
    "sql_dim_transachist = \"SELECT ProductID, TransactionDate, Quantity FROM transactionhistory;\"\n",
    "df_dim_transachist = get_sql_dataframe(sql_dim_transachist, **mysql_args)\n",
    "df_dim_transachist.insert(0, \"dim_transacthist_key\", range(1, df_dim_transachist.shape[0]+1))\n",
    "df_dim_transachist.TransactionDate = df_dim_transachist.TransactionDate.astype('datetime64[ns]').dt.date\n",
    "\n",
    "df_dim_transachist = pd.merge(df_dim_transachist, df_dim_date, left_on=\"TransactionDate\", right_on=\"Date\", how=\"inner\")\n",
    "df_dim_transachist = df_dim_transachist.rename(columns={\"DateKey\":\"TransactionDateKey\"})\n",
    "df_dim_transachist.drop(columns=['TransactionDate','Date'],inplace=True)\n",
    "\n",
    "# put transachist dim table in MySQL for analytics\n",
    "dataframe = df_dim_transachist\n",
    "table_name = 'dim_transachist'\n",
    "primary_key = 'dim_transacthist_key'\n",
    "db_operation = \"insert\"\n",
    "\n",
    "set_dataframe(dataframe, table_name, primary_key, db_operation, **mysql_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9904ce41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [dim_transacthist_key, ProductID, Quantity, TransactionDateKey, StartDateKey, EndDateKey]\n",
      "Index: []\n",
      "(0, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\n    Results analysis:\\n    Looking at more records (than 5) of df_product_qtys would give insight into\\n    which products are growing in popularity (i.e. higher quantities purchased) over time.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insight 2, continued:\n",
    "sql_price_changes = \"\"\"\n",
    "    SELECT\n",
    "        t.*,\n",
    "        p.StartDateKey,\n",
    "        p.EndDateKey\n",
    "    FROM fact_productcosthist p\n",
    "    LEFT JOIN dim_transachist t\n",
    "        ON p.ProductID = t.ProductID\n",
    "    WHERE\n",
    "        ((TransactionDateKey >= StartDateKey) AND (TransactionDateKey <= EndDateKey));\n",
    "\"\"\"\n",
    "df_price_changes = get_sql_dataframe(sql_price_changes, **mysql_args)\n",
    "print(df_price_changes.head(5))\n",
    "print(df_price_changes.shape)\n",
    "\n",
    "\"\"\" \n",
    "    Results analysis:\n",
    "    Dataframe df_price_changes was meant to show the transactions that occurred\n",
    "    where the transaction took place during one of the time periods where the product price changed.\n",
    "    The dataframe appeared empty, and after checking its shape, there are in fact\n",
    "    no records where a product in the transaction history was purchased at a former price from productcosthistory.\n",
    "\n",
    "    I then noticed that all of the TransactionDateKeys were values later than the EndDateKeys, meaning\n",
    "    none of the transactions in transactionhistory took place in the periods where the product price was changing.\n",
    "\n",
    "    As a result, I would not be able to sum the quantities purchased of each product as the price changed as planned\n",
    "    (but wanted to demonstrate my work above). I instead shifted to investigate quantities purchased of each product on each transaction date.\n",
    "\"\"\"\n",
    "\n",
    "sql_product_qtys = \"\"\"\n",
    "    SELECT\n",
    "        t.TransactionDateKey,\n",
    "        p.ProductID,\n",
    "        p.Name AS ProductName,\n",
    "        SUM(t.Quantity) AS TotalProductQty\n",
    "    FROM fact_productcosthist p\n",
    "    JOIN dim_transachist t\n",
    "        ON p.ProductID = t.ProductID\n",
    "    GROUP BY\n",
    "        p.ProductID,\n",
    "        p.Name,\n",
    "        t.TransactionDateKey;\n",
    "\"\"\"\n",
    "df_product_qtys = get_sql_dataframe(sql_product_qtys, **mysql_args)\n",
    "df_product_qtys.head(5)\n",
    "\n",
    "\"\"\" \n",
    "    Results analysis:\n",
    "    Looking at more records (than 5) of df_product_qtys would give insight into\n",
    "    which products are growing in popularity (i.e. higher quantities purchased) over time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd57f2d",
   "metadata": {},
   "source": [
    "## Additions for Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931dcef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Final/capstone: analyze how managers' employees perform as orders come in\n",
    "- Stream order data using fact_salesorder, persisted to MySQL in midterm's half of the notebook\n",
    "    - Achieve the 'real-time' data by querying for all fact_salesorder results, saving to 3 jsons (called 'orders_01.json', for example) in a streaming/\n",
    "- Join with table of managers' names\n",
    "- Join with the table of employee details view\n",
    "- Join with salesperson table\n",
    "- Now have info on: for each incoming sales order, which manager that salesperson works under\n",
    "    - Final, organized output: show managers by name, the total sales last year of their salespeople who are generating new sales orders,\n",
    "    and calculation of how much return on investment (in the form of salesperson bonus) that employee is generating for them.\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "# data_dir = os.path.join(base_dir, 'retail-org')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "orders_stream_dir = os.path.join(stream_dir, 'orders')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"retail_dlh\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "orders_output_bronze = os.path.join(database_dir, 'fact_orders', 'bronze')\n",
    "orders_output_silver = os.path.join(database_dir, 'fact_orders', 'silver')\n",
    "orders_output_gold = os.path.join(database_dir, 'fact_orders', 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2bf40d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jocelyn:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Final Project in Juptyer</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a1a7fbf1a0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "shuffle_partitions = int(os.cpu_count())\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('PySpark Final Project in Juptyer')\\\n",
    "    .master(worker_threads)\\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '2g')\\\n",
    "    .config('spark.sql.catalog.spark_catalog', 'org.apache.spark.sql.delta.catalog.DeltaCatalog') \\\n",
    "    .config('spark.sql.extensions', 'io.delta.sql.DeltaSparkSessionExtension') \\\n",
    "    .config('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .config('spark.sql.debug.maxToStringFields', 50) \\\n",
    "    .config('spark.sql.shuffle.partitions', shuffle_partitions) \\\n",
    "    .config('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .config('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .config('spark.sql.warehouse.dir', database_dir) \\\n",
    "    .config('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4fe6ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the database\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 Final Project Database'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Final');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc0f6d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_key</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Engineering Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Marketing Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>Production Control Manager</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_key  employee_id                       title\n",
       "0            1            3         Engineering Manager\n",
       "1            2            6           Marketing Manager\n",
       "2            3           21  Production Control Manager"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting dimensions\n",
    "# 1: Getting batch/static data (**local folder**)\n",
    "# Table of managers achieved by running the following in MySQL Workbench, \n",
    "# exporting and saving result as json in /data/batch/\n",
    "\"\"\"\n",
    "    USE adventureworks;\n",
    "\n",
    "    SELECT EmployeeID, Title FROM employee WHERE Title LIKE \"%Manager%\";\n",
    "\"\"\"\n",
    "get_file_info(batch_dir)\n",
    "\n",
    "manager_csv = os.path.join(batch_dir,'managers.csv')\n",
    "df_dim_managers = spark.read.format('csv').options(header='true',inferSchema='true').load(manager_csv)\n",
    "\n",
    "# 2: Cleaning up dataframe\n",
    "# Lowercase all columns\n",
    "df_dim_managers = df_dim_managers.withColumnsRenamed({'EmployeeID':'employee_id','Title':'title'})\n",
    "\n",
    "# add a PK\n",
    "df_dim_managers.createOrReplaceTempView(\"managers\")\n",
    "sql_managers = \"\"\"\n",
    "                    SELECT \n",
    "                        *, ROW_NUMBER() OVER (ORDER BY employee_id) AS manager_key \n",
    "                    FROM \n",
    "                        managers;\n",
    "                \"\"\"\n",
    "df_dim_managers = spark.sql(sql_managers)\n",
    "\n",
    "# reorder columns\n",
    "reordered_cols = df_dim_managers.columns\n",
    "reordered_cols.remove('manager_key')\n",
    "reordered_cols.insert(0,'manager_key')\n",
    "df_dim_managers = df_dim_managers[reordered_cols]\n",
    "\n",
    "df_dim_managers.toPandas().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d8e0a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         manager_key|                 int|   NULL|\n",
      "|         employee_id|                 int|   NULL|\n",
      "|               title|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|          retail_dlh|       |\n",
      "|               Table|        dim_managers|       |\n",
      "|        Created Time|Wed Dec 03 04:41:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 4.0.1|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/c:/Users/jo...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_key</th>\n",
       "      <th>employee_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Engineering Manager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>Marketing Manager</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   manager_key  employee_id                title\n",
       "0            1            3  Engineering Manager\n",
       "1            2            6    Marketing Manager"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3: Save manager data to the DLH\n",
    "df_dim_managers.write.saveAsTable(f'{dest_database}.dim_managers',mode='overwrite')\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_managers;\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_managers LIMIT 2;\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eabaf47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>NationalIDNumber</th>\n",
       "      <th>LoginID</th>\n",
       "      <th>ManagerID</th>\n",
       "      <th>FirstName</th>\n",
       "      <th>MiddleName</th>\n",
       "      <th>LastName</th>\n",
       "      <th>Title</th>\n",
       "      <th>EmailAddress</th>\n",
       "      <th>EmailPromotion</th>\n",
       "      <th>Phone</th>\n",
       "      <th>BirthDate</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Gender</th>\n",
       "      <th>HireDate</th>\n",
       "      <th>SalariedFlag</th>\n",
       "      <th>VacationHours</th>\n",
       "      <th>SickLeaveHours</th>\n",
       "      <th>CurrentFlag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14417807</td>\n",
       "      <td>adventure-works\\guy1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>Guy</td>\n",
       "      <td>R</td>\n",
       "      <td>Gilbert</td>\n",
       "      <td>Production Technician - WC60</td>\n",
       "      <td>guy1@adventure-works.com</td>\n",
       "      <td>0</td>\n",
       "      <td>320-555-0195</td>\n",
       "      <td>1972-05-15 00:00:00</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>1996-07-31 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>253022876</td>\n",
       "      <td>adventure-works\\kevin0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Kevin</td>\n",
       "      <td>F</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Marketing Assistant</td>\n",
       "      <td>kevin0@adventure-works.com</td>\n",
       "      <td>2</td>\n",
       "      <td>150-555-0189</td>\n",
       "      <td>1977-06-03 00:00:00</td>\n",
       "      <td>S</td>\n",
       "      <td>M</td>\n",
       "      <td>1997-02-26 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>509647174</td>\n",
       "      <td>adventure-works\\roberto0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Roberto</td>\n",
       "      <td>None</td>\n",
       "      <td>Tamburello</td>\n",
       "      <td>Engineering Manager</td>\n",
       "      <td>roberto0@adventure-works.com</td>\n",
       "      <td>0</td>\n",
       "      <td>212-555-0187</td>\n",
       "      <td>1964-12-13 00:00:00</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>1997-12-12 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EmployeeID NationalIDNumber                   LoginID  ManagerID FirstName  \\\n",
       "0           1         14417807      adventure-works\\guy1       16.0       Guy   \n",
       "1           2        253022876    adventure-works\\kevin0        6.0     Kevin   \n",
       "2           3        509647174  adventure-works\\roberto0       12.0   Roberto   \n",
       "\n",
       "  MiddleName    LastName                         Title  \\\n",
       "0          R     Gilbert  Production Technician - WC60   \n",
       "1          F       Brown           Marketing Assistant   \n",
       "2       None  Tamburello           Engineering Manager   \n",
       "\n",
       "                   EmailAddress  EmailPromotion         Phone  \\\n",
       "0      guy1@adventure-works.com               0  320-555-0195   \n",
       "1    kevin0@adventure-works.com               2  150-555-0189   \n",
       "2  roberto0@adventure-works.com               0  212-555-0187   \n",
       "\n",
       "             BirthDate MaritalStatus Gender             HireDate SalariedFlag  \\\n",
       "0  1972-05-15 00:00:00             M      M  1996-07-31 00:00:00            0   \n",
       "1  1977-06-03 00:00:00             S      M  1997-02-26 00:00:00            0   \n",
       "2  1964-12-13 00:00:00             M      M  1997-12-12 00:00:00            1   \n",
       "\n",
       "   VacationHours  SickLeaveHours CurrentFlag  \n",
       "0             21              30           1  \n",
       "1             42              41           1  \n",
       "2              2              21           1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second dimension\n",
    "# Get employee details data from **MongoDB**\n",
    "\n",
    "query = {}\n",
    "mongodb_args['collection'] = 'employee_details'\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "df_dim_employee_detail = get_mongo_dataframe(client, mongodb_args['db_name'],'employee_details',query)\n",
    "# df_dim_employee_detail = get_mongodb_dataframe(spark, **mongodb_args)\n",
    "\n",
    "df_dim_employee_detail.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df724ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalesPersonID</th>\n",
       "      <th>TerritoryID</th>\n",
       "      <th>SalesQuota</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>CommissionPct</th>\n",
       "      <th>SalesYTD</th>\n",
       "      <th>SalesLastYear</th>\n",
       "      <th>rowguid</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>268</td>\n",
       "      <td>NULL</td>\n",
       "      <td>NULL</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.775585e+05</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>2001-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>275</td>\n",
       "      <td>2</td>\n",
       "      <td>300000</td>\n",
       "      <td>4100</td>\n",
       "      <td>0.012</td>\n",
       "      <td>4.557045e+06</td>\n",
       "      <td>1.750406e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2001-06-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>276</td>\n",
       "      <td>4</td>\n",
       "      <td>250000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>5.200475e+06</td>\n",
       "      <td>1.439156e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2001-06-24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SalesPersonID TerritoryID SalesQuota  Bonus  CommissionPct      SalesYTD  \\\n",
       "0            268        NULL       NULL      0          0.000  6.775585e+05   \n",
       "1            275           2     300000   4100          0.012  4.557045e+06   \n",
       "2            276           4     250000   2000          0.015  5.200475e+06   \n",
       "\n",
       "   SalesLastYear rowguid ModifiedDate  \n",
       "0   0.000000e+00     ...   2001-01-28  \n",
       "1   1.750406e+06     ...   2001-06-24  \n",
       "2   1.439156e+06     ...   2001-06-24  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third dimension\n",
    "# Salesperson dimension table, retrieved from local file system (including more data than used in midterm)\n",
    "salesperson_csv_path = os.path.join(batch_dir, 'salesperson.csv')\n",
    "df_dim_salesperson = spark.read.format('csv').options(inferSchema='true',header='true').load(salesperson_csv_path)\n",
    "\n",
    "df_dim_salesperson.toPandas().head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f51fce",
   "metadata": {},
   "source": [
    "### Bronze Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae34bfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name, size, modification_time]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# orders fact table\n",
    "get_file_info(stream_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9fe06cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_bronze = (\n",
    "    spark.readStream \\\n",
    "    .option(\"schemaLocation\",orders_output_bronze) \\\n",
    "    .option(\"maxFilesPerTrigger\",1) \\\n",
    "    .option(\"multiLine\",\"true\") \\\n",
    "    .json(orders_stream_dir)\n",
    ")\n",
    "\n",
    "df_orders_bronze.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c631717b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: c4c6dee1-711b-421e-ae32-8eab078146f8\n",
      "Query Name: orders_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "# Writing real-time order data to parquet file\n",
    "orders_checkpoint_bronze = os.path.join(orders_output_bronze, '_checkpoint')\n",
    "\n",
    "orders_bronze_query = (\n",
    "    df_orders_bronze \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_bronze\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_bronze) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_bronze)\n",
    ")\n",
    "\n",
    "print(f\"Query ID: {orders_bronze_query.id}\")\n",
    "print(f\"Query Name: {orders_bronze_query.name}\")\n",
    "print(f\"Query Status: {orders_bronze_query.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9eb98fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_bronze_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845e24b",
   "metadata": {},
   "source": [
    "### Silver Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "993f7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining streaming and batch data\n",
    "df_orders_silver = spark.readStream.format(\"parquet\").load(orders_output_bronze) \\\n",
    "    .join(df_dim_salesperson,'SalesPersonID') \\\n",
    "    .select(\n",
    "        col('fact_salesorder_key'),\n",
    "        col('SalesOrderID'),\n",
    "        col('SalesPersonID'),\n",
    "        df_dim_salesperson.Bonus,\n",
    "        df_dim_salesperson.SalesLastYear,\n",
    "        col('OrderDateKey'),\n",
    "        col('ShipDateKey')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8336cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orders_silver.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc35c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fact_salesorder_key: long (nullable = true)\n",
      " |-- SalesOrderID: long (nullable = true)\n",
      " |-- SalesPersonID: long (nullable = true)\n",
      " |-- Bonus: integer (nullable = true)\n",
      " |-- SalesLastYear: double (nullable = true)\n",
      " |-- OrderDateKey: long (nullable = true)\n",
      " |-- ShipDateKey: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_orders_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30c3a107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing joined sales order and salesperson data into DLH\n",
    "orders_checkpoint_silver = os.path.join(orders_output_silver, '_checkpoint')\n",
    "\n",
    "orders_silver_query = (\n",
    "    df_orders_silver.writeStream \\\n",
    "    .format(\"parquet\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .queryName(\"orders_silver\")\n",
    "    .trigger(availableNow = True) \\\n",
    "    .option(\"checkpointLocation\", orders_checkpoint_silver) \\\n",
    "    .option(\"compression\", \"snappy\") \\\n",
    "    .start(orders_output_silver)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b4602d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_silver_query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d85efe6",
   "metadata": {},
   "source": [
    "### Gold Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3308431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to csv, then read from it to make dim_employee into spark df (for joining with stream data)\n",
    "dim_employee_csv_path = os.path.join(batch_dir,'employee_detail.csv')\n",
    "df_dim_employee_detail.to_csv(dim_employee_csv_path)\n",
    "df_dim_employee_detail = spark.read.format('csv').options(inferSchema=\"true\",header=\"true\").load(dim_employee_csv_path)\n",
    "\n",
    "df_ret_on_employee_gold = spark.readStream.format('parquet').load(orders_output_silver) \\\n",
    "    .join(df_dim_employee_detail, df_dim_employee_detail.EmployeeID==col('SalesPersonID')) \\\n",
    "    .join(df_dim_managers, df_dim_employee_detail.ManagerID==df_dim_managers.employee_id, 'left_outer') \\\n",
    "    .groupBy(df_dim_employee_detail.ManagerID, df_dim_employee_detail.FirstName, df_dim_employee_detail.LastName) \\\n",
    "    .agg(sum(col('SalesLastYear')).alias('return_on_their_employees')) \\\n",
    "    .select(\n",
    "        df_dim_employee_detail.ManagerID,\n",
    "        col('FirstName').alias('manager_first_nm'),\n",
    "        col('LastName').alias('manager_last_nm'),\n",
    "        col('return_on_their_employees')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20e9e827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ManagerID: double (nullable = true)\n",
      " |-- manager_first_nm: string (nullable = true)\n",
      " |-- manager_last_nm: string (nullable = true)\n",
      " |-- return_on_their_employees: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ret_on_employee_gold.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebfc194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_gold_query = (\n",
    "    df_ret_on_employee_gold.writeStream \\\n",
    "    .format(\"memory\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .queryName(\"fact_ret_on_employee\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "347559c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stream has processed 1 batchs\n"
     ]
    }
   ],
   "source": [
    "wait_until_stream_is_ready(orders_gold_query, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "042e6584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ManagerID: double (nullable = true)\n",
      " |-- manager_first_nm: string (nullable = true)\n",
      " |-- manager_last_nm: string (nullable = true)\n",
      " |-- return_on_their_employees: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact_ret_on_employee = spark.sql(\"SELECT * FROM fact_ret_on_employee\")\n",
    "df_fact_ret_on_employee.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4405241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_ret_on_employee_gold_final = df_fact_ret_on_employee \\\n",
    ".select(col(\"ManagerID\").alias('manager_id'), \\\n",
    "        col(\"manager_first_nm\"), \\\n",
    "        col(\"manager_last_nm\"), \\\n",
    "        col(\"return_on_their_employees\")) \\\n",
    ".orderBy(desc(\"return_on_their_employees\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46eb1f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>manager_id</th>\n",
       "      <th>manager_first_nm</th>\n",
       "      <th>manager_last_nm</th>\n",
       "      <th>return_on_their_employees</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>273.0</td>\n",
       "      <td>Stephen</td>\n",
       "      <td>Jiang</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>273.0</td>\n",
       "      <td>Syed</td>\n",
       "      <td>Abbas</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Tete</td>\n",
       "      <td>Mensa-Annan</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>273.0</td>\n",
       "      <td>Amy</td>\n",
       "      <td>Alberts</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>284.0</td>\n",
       "      <td>Ranjit</td>\n",
       "      <td>Varkey Chudukatil</td>\n",
       "      <td>2.899813e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Pamela</td>\n",
       "      <td>Ansman-Wolfe</td>\n",
       "      <td>1.638000e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Jillian</td>\n",
       "      <td>Carson</td>\n",
       "      <td>7.709139e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Tsvi</td>\n",
       "      <td>Reiter</td>\n",
       "      <td>6.603218e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Michael</td>\n",
       "      <td>Blythe</td>\n",
       "      <td>6.546520e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Garrett</td>\n",
       "      <td>Vargas</td>\n",
       "      <td>3.029918e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>268.0</td>\n",
       "      <td>David</td>\n",
       "      <td>Campbell</td>\n",
       "      <td>2.139751e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Linda</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>4.921914e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>268.0</td>\n",
       "      <td>José</td>\n",
       "      <td>Saraiva</td>\n",
       "      <td>4.463734e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>284.0</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>Valdez</td>\n",
       "      <td>9.940418e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>288.0</td>\n",
       "      <td>Lynn</td>\n",
       "      <td>Tsoflias</td>\n",
       "      <td>1.321558e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>284.0</td>\n",
       "      <td>Jae</td>\n",
       "      <td>Pak</td>\n",
       "      <td>4.351290e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>268.0</td>\n",
       "      <td>Shu</td>\n",
       "      <td>Ito</td>\n",
       "      <td>4.084807e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    manager_id manager_first_nm    manager_last_nm  return_on_their_employees\n",
       "0        273.0          Stephen              Jiang               0.000000e+00\n",
       "1        273.0             Syed              Abbas               0.000000e+00\n",
       "2        268.0             Tete        Mensa-Annan               0.000000e+00\n",
       "3        273.0              Amy            Alberts               0.000000e+00\n",
       "4        284.0           Ranjit  Varkey Chudukatil               2.899813e+08\n",
       "5        268.0           Pamela       Ansman-Wolfe               1.638000e+08\n",
       "6        268.0          Jillian             Carson               7.709139e+08\n",
       "7        268.0             Tsvi             Reiter               6.603218e+08\n",
       "8        268.0          Michael             Blythe               6.546520e+08\n",
       "9        268.0          Garrett             Vargas               3.029918e+08\n",
       "10       268.0            David           Campbell               2.139751e+08\n",
       "11       268.0            Linda           Mitchell               4.921914e+08\n",
       "12       268.0             José            Saraiva               4.463734e+08\n",
       "13       284.0           Rachel             Valdez               9.940418e+07\n",
       "14       288.0             Lynn           Tsoflias               1.321558e+08\n",
       "15       284.0              Jae                Pak               4.351290e+08\n",
       "16       268.0              Shu                Ito               4.084807e+08"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact_ret_on_employee_gold_final.write.saveAsTable(f\"{dest_database}.fact_ret_on_employee\", mode=\"overwrite\")\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.fact_ret_on_employee\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a28488d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysparkenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
